% pdflatex notes.tex 

\documentclass[twocolumn]{article}
\usepackage{amsfonts}
\usepackage{pifont}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[most]{tcolorbox}
\usepackage{empheq}
\usepackage{geometry}
\usepackage{amsthm}
\geometry{a4paper, margin=1cm, includefoot}

\theoremstyle{plain}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]

\newtcbox{\mymath}[1][]{%
    nobeforeafter, math upper, tcbox raise base,
    enhanced, colframe=blue!30!black,
    colback=blue!15, boxrule=1pt,
    #1}

\begin{document}

\title{Notes on \href{https://www.cs.mcgill.ca/~wlh/grl_book/files/GRL_Book.pdf}{Graph Representation Learning}\\\small{Chapters 1 - 2}}
\author{Rom Parnichkun}

\maketitle

\section{Representing Graphs}

Formally, a graph is defined as follows.
\begin{equation}
\mathcal{G} = (\mathcal{V}, \mathcal{E}).
\end{equation}
In which, $\mathcal{V}$ and $\mathcal{E}$ are the nodes and edges of the graphs. Edges going from $u \in \mathcal{V}$ to $v \in \mathcal{V}$ are represented as $(u,v) \in \mathcal{E}$.

Graphs can be represented using an \textit{adjacency matrix} $\textbf{A} \in \mathbb{R}^{|\mathcal{V}|x|\mathcal{V}|}$, in which $A[u,v]$ represent $(u,v) \in \mathcal{E}$. Given output unit of every node $\mathbf{u} \in \mathbb{R}^{|V|}$, $\mathbf{s} = \mathbf{A}\mathbf{u}$ is the combined weighed signal from every adjacent node. Moreover, $\mathbf{A}^i[u,v]$ represents the number of $i$-length paths that connects $u$ and $v$.

Nodes within a graph may have \textit{features} or \textit{attributes} associated with them. They can be represented with matrix $\textbf{X} \in \mathbb{R}^{|V|\times m}$, for nodes with $m$ features.

\section{Types of Graphs}

\begin{itemize}
    \item \label{simple_graph} \textbf{Simple graphs} are graphs with at most one edge between each pair of nodes, and all edges are undirected.
    \item \textbf{Heterogeneous graphs}: Nodes have types, and edges satisfy constraints according to those types.
    \item \textbf{Multiplex graphs}: Graph is decomposed in a set of k \textit{layers}. Every node is assumed to belong to every layer, and each layer corresponds to a unique relation.
\end{itemize}

\section{Machine Learning on Graphs}

The following lists the types of tasks that may be done on graphs.

\begin{itemize}
    \item \textbf{Node classification}: Classify a node based on its features and relations to other nodes. Ideas such as \textit{homophily}, which is the tendency for nodes to share attributes with their neighbors; \textit{structural equivalence}, which is the idea that nodes with similar local neighborhood structures will have similar labels; and \textit{heterophily}, which presumes that nodes will be preferentially connected to nodes with different labels, have been used to assist node classification.
    \item \textbf{Relation prediction}: Also known as link prediction, graph completion, relational inference, is the task of inferring edges between nodes in a graph.
    \item \textbf{Clustering and community detection}: Finding subgroups that may be useful.
    \item \textbf{Graph classification, regression, and clustering}: Graphs are treated as data points, which is to be classified, regressed, or clustered.
\end{itemize}

\section{Graph Statistics and Features}

The following lists node-level statistics and features.
\begin{itemize}
    \item \textbf{Node degree}: The number of edges incident to a node.
        \begin{equation} \label{degree}
            d[u] = \sum_{v \in V}\textbf{A}[u,v],
        \end{equation}
        \begin{equation}
            \mathbf{d} = \mathbf{A}\mathbf{1}_{|V|},
        \end{equation}
        in which, $\mathbf{1}_{|V|} = (1,...,1) \in \mathbb{R}^{|V|}$.
    \item \textbf{Node centrality}: \textit{Eigenvector centrality} is a measure of node importance. The eigenvector $\mathbf{e}$ corresponding to the largest eigenvalue $\lambda$ contains the importance of each node on its indices.
        \begin{equation}
            \lambda \mathbf{e} = \mathbf{A}\mathbf{e}
        \end{equation}
        \begin{equation}
            \mathbf{e}[u] = \frac{1}{\lambda}\sum_{v \in V}\mathbf{A}[u,v]\mathbf{e}[v].
        \end{equation}
    \item \textbf{Clustering coefficient}: Measures the degree of clustering for a node. It is the ratio between the number of adjacent node pairs that are also adjacent to each other by the total combination of node pairs for $u$.
        \begin{equation}
            c[u] = \frac{|(v_1, v_2) \in \mathcal{E}:v_1, v_2 \in \mathcal{N}(u)|}{\binom{d[u]}{2}}.
        \end{equation}
    An alternative way of viewing the clustering coefficient is that it counts the number of closed triangles that is connected to a particular node.
\end{itemize}

The following lists graph-level features and kernels.
\begin{itemize}
    \item \textbf{Bag of nodes}: An aggregated view of node-level representations. Such as histograms of degrees, centralities, and clustering coefficients of the nodes in the graph.
    \item \textbf{Weisfeiler-Lehman (WL)}: Is a kernel and an algorithm that iteratively aggregates the neighborhood to find statistics beyond the immediate neighbor of a node. The algorithm is as follows:
        \begin{enumerate}
            \item Assign an initial label to each node. In many cases, this is the degree of each node; $l_0(v) = d[v]$.
            \item Next, a new label is iteratively assigned with an aggregation function.
                \begin{equation}
                    l_i(v) = \text{AGGR}\bigl( \{ l_{i-1}(u) \; \forall u \in \mathcal{N}(v) \}\bigr)
                \end{equation}
            \item After $K$ iterations we have $l_K(v)$, which is a K-hop summary of every node. Which can be used to compare nodes at a higher level.
        \end{enumerate}
    \item \textbf{Graphlets}: Graphlets are subgraph structures that may commonly exhibit within larger graphs. The number of these different structures can be treated as graph-level features.
\end{itemize}

The following lists statistics on node-to-node relationships within graphs.
\begin{itemize}
    \item \textbf{Similarity matrix}: $\mathbf{S} \in \mathbb{R}^{|V|\times |V|}$ is a matrix containing the number of shared neighbors of each node pairs.
        \begin{equation}
            \mathbf{S}[u,v] = |\mathcal{N}(u)\cap\mathcal{N}(v)|.
        \end{equation}
    \item \textbf{Sorenson index}: Similarity matrix with normalization.
        \begin{equation}
            \mathbf{S}_{\text{Sorenson}}[u,v] = \frac{2\mathbf{S}[u,v]}{d[u] + d[v]}.
        \end{equation}
    \item \textbf{Salton index}: Similarity matrix with normalization.
        \begin{equation}
            \mathbf{S}_{\text{Salton}}[u,v] = \frac{2\mathbf{S}[u,v]}{\sqrt{d[u]d[v]}}.
        \end{equation}
    \item \textbf{Jaccard index}: Similarity matrix with normalization.
        \begin{equation}
            \mathbf{S}_{\text{Jaccard}}[u,v] = \frac{\mathbf{S}[u,v]}{|\mathcal{N}(u) \cup \mathcal{N}(v)|}.
        \end{equation}
    \item \textbf{Resource allocation (RA) index}: Counts the inverse degrees of common neighbors,
        \begin{equation}
            \mathbf{S}_{\text{RA}}[v_1, v_2] = \sum_{u \in \mathcal{N}(v_1) \cap \mathcal{N}(v_2)}{\frac{1}{d[u]}}
        \end{equation}
    \item \textbf{Adamic-Adar (AA) index}: Similar to RA index but using the inverse log.
        \begin{equation}
            \mathbf{S}_{\text{AA}}[v_1, v_2] = \sum_{u \in \mathcal{N}(v_1) \cap \mathcal{N}(v_2)}{\frac{1}{\text{log}(d[u])}}
        \end{equation}
    \item \textbf{Katz index}: Counts the number of paths of all lengths between a pair of nodes.
        \begin{equation}
            \mathbf{S}_{\text{Katz}}[u,v] = \sum_{i=1}^{\infty}{\beta^i\mathbf{A}^i[u,v]},
        \end{equation}
        where $\beta \in \mathbb{R}^{+}$ is a user-defined parameter controlling how much weight is given to short versus long paths.
    \item \textbf{LHN similarity}: A normalized version of the Katz index, which reduces a high-degree bias in the Katz index.
        \begin{equation}
            \mathbf{S}_{\text{LNH}}[u,v] = \mathbf{I}[u,v] + \frac{2m}{d[u]d[v]} \sum_{i=0}^{\infty}\beta^i\lambda^{1-i}_1 \mathbf{A}^i[u,v],
        \end{equation}
        in which $\lambda_1$ is the largest eigenvalue of $\mathbf{A}$.
    \item \textbf{Random walk similarity}: Similarity between two nodes is proportional to how likely we are to reach each node from random walk starting from the other node.
        \begin{equation}
            \mathbf{P} = \mathbf{A}\mathbf{D}^{-1}.
        \end{equation}
        Here $\mathbf{P[u,v]}$ is a stochastic adjacency matrix, with adjacency probability scaled proportional to the node's inverse degree.
        \begin{equation}
            \mathbf{q}_u = c\mathbf{P}\mathbf{q}_u + (1-c)\mathbf{e}_u.
        \end{equation}
        This implicit equation models the probability of reaching each node with a random walk policy. The $c$ term determines the probability that the random walk restarts at $u$ and $\mathbf{e}_u$ is a one-hot indicator vector for node $u$. The solution to this recurrence is as follows.
        \begin{equation}
            \mathbf{q}_u = (1-c)(\mathbf{I}-c\mathbf{P})^{-1}\mathbf{e}_u.
        \end{equation}
        Finally, random walk similarity formulated as follows.
        \begin{equation}
            \mathbf{S}_{\text{RW}}[u,v] = \mathbf{q}_u[v] + \mathbf{q}_v[u].
        \end{equation}
\end{itemize}

\section{Graph Laplacians}

Graph \textit{Laplacians} have several useful mathematical properties that can help in tasks such as clustering. For more information read about \textbf{Spectral Graph Theory}.

\subsection{Unnormalized Laplacian}

\begin{equation}
    \mathbf{L} = \mathbf{D} - \mathbf{A},
\end{equation}
where $\mathbf{D}$ is  matrix with node degrees (see Equation \ref{degree}) on its diagonals. The Laplacian summarizes several important properties of the graph including,

\begin{enumerate}
    \item It is symmetric and positive semi-definite.
        \begin{equation}
            \mathbf{x}^T\mathbf{L}\mathbf{x} \geq 0, \forall \mathbf{x} \in \mathbb{R}^{|V|}
        \end{equation}
    \item The following vector identity holds $\forall \mathbf{x} \in \mathbb{R}^{|V|}$
        \begin{eqnarray}
            \mathbf{x}^T\mathbf{L}\mathbf{x} &=& \frac{1}{2}\sum_{u \in \mathcal {V}}\sum_{v \in \mathcal{V}}\mathbf{A}[u,v](\mathbf{x}[u]-\mathbf{x}[v])^2\\
            &=& \sum_{(u,v) \in \mathcal{E}}(\mathbf{x}[u]-\mathbf{x}[v])^2
        \end{eqnarray}
    \item $\mathbf{L}$ has $|V|$ non-negative eigenvalues: $0=\lambda_{|V|} \leq \lambda_{|V|-1} \leq ... \leq \lambda_1$
\end{enumerate}

\subsection{Normalized Laplacian}

\begin{itemize}
    \item \textbf{Symmetric normalized Laplacian}: 
        \begin{equation}
            \mathbf{L}_{\text{sym}} = \mathbf{D}^{-\frac{1}{2}}\mathbf{L}\mathbf{D}^{-\frac{1}{2}}
        \end{equation}
    \item \textbf{Random walk Laplacian}:
        \begin{equation}
            \mathbf{L}_{\text{RW}} = \mathbf{D}^{-1}\mathbf{L}
        \end{equation}
\end{itemize}

\begin{theorem} \label{thm:connected_components}
    The geometric multiplicity of the 0 eigenvalue (the dimension of the subspace spanned by the eigenvectors associated with $\lambda = 0$) of the Laplacian $\mathbf{L}$ corresponds to the number of connected components in the graph.
\end{theorem}
\begin{proof}
    This can be seen by noting that for any eigenvector $\mathbf{e}$ of the eigenvalue $0$ we have that 
    \begin{equation} \label{eq:ele}
        \mathbf{e}^T\mathbf{L}\mathbf{e} = 0
    \end{equation}
    by the definition of the eigenvalue-eigenvector equation. And, the result in Equation \ref{eq:ele} implies that 
    \begin{equation}
        \sum_{(u,v) \in \mathcal{E}}(\mathbf{e}[u] - \mathbf{e}[v])^2 = 0.
    \end{equation}
    Therefore, $\mathbf{e}[u] = \mathbf{e}[v], \forall(u,v) \in \mathcal{E}$, hence $\mathbf{e}[u]$ is the same constant for all nodes $u$ that are in the same connected component.

    If the graph is fully connected, the eigenvector for $\lambda=0$ will be a constant vector of ones $\mathbf{e} = \mathbf{1}_{|V|}$. Conversely, if the graph is composed of multiple connected components, then we will have blocks of the Laplacian that corresponds to each connected components that satisfies $\lambda_k = 0$ and $\mathbf{e}_k = \mathbf{1}_{|V_k|}$. Therefore, the number of these blocks corresponds to the number of connected components in the graph.
\end{proof}

\subsection{Graph cuts and clustering}

Theorem \ref{thm:connected_components} can be used to create clusters of disconnected components. This section shows that the Laplacian can be used to give an optimal clustering of nodes \textit{within a fully connected graph}.

\subsubsection{Graph cuts}

An optimal cluster is defined formally as the minimization of the following equation.
\begin{equation}
    \text{cut}(\mathcal{A}_1, ..., \mathcal{A}_K) = \frac{1}{2}\sum_{k=1}^{K}|(u,v)\in \mathcal{E} : u \in \mathcal{A}_k, v \in \bar{\mathcal{A}}_k|,
\end{equation}
which is simply the count of how many edges cross the boundary between the partition of nodes. However, simply minimizing this can create a trivial solution of having a single node per cluster. Therefore we can instead minimize the \textit{Ratio cut}:
\begin{equation}
    \text{RatioCut}(\mathcal{A}_1, ..., \mathcal{A}_K) = \frac{1}{2}\sum_{k=1}^K\frac{|(u,v)\in \mathcal{E} : u \in \mathcal{A}_k, v\in \bar{\mathcal{A}}_k|}{|\mathcal{A}_k|}.
\end{equation}
Another popular choice is minimizing the \textit{Normalized Cut (NCut)}:
\begin{equation}
    \text{NCut}(\mathcal{A}_1, ..., \mathcal{A}_K) = \frac{1}{2}\sum_{k=1}^K\frac{|(u,v)\in \mathcal{E} : u \in \mathcal{A}_k, v\in \bar{\mathcal{A}}_k|}{\text{vol}(\mathcal{A}_k)},
\end{equation}
where $\text{vol}(\mathcal{A}) = \sum_{u \in \mathcal{A}}d[u]$. The NCut enforces that all clusters have a similar number of edges incident to their nodes.

\subsubsection{Approximating the RatioCut with Laplacian spectrum}

Setting 
\begin{equation}
    \mathbf{a} = \begin{cases}
        \sqrt{\frac{|\bar{\mathcal{A}}|}{\mathcal{A}}} & \text{if} \; u \in \mathcal{A} \\
        -\sqrt{\frac{|\mathcal{A}|}{\mathcal{A}}} & \text{if} \; u \in \bar{\mathcal{A}}
    \end{cases},
\end{equation}
we find that 
\begin{equation}
    \mathbf{a}^T \mathbf{L}\mathbf{a} = |\mathcal{V}|\text{RatioCut}(\mathcal{A}, \bar{\mathcal{A}}).
\end{equation}
By the Rayleigh-Ritz Theorem, the solution to $\min_{\mathbf{a} \in \mathbb{R}^{|\mathcal{V}|}}\mathbf{a}^T\mathbf{L}\mathbf{a}$ is given by the second-smallest eigenvector of $\mathbf{L}$ (since the smallest eigenvector is equal to $\mathbf{1}$. Then clustering can be done as follows:
\begin{equation}
    \begin{cases}
        u \in \mathcal{A} & \text{if} \; \mathbf{a}[u] \geq 0 \\
        u \in \bar{\mathcal{A}} & \text{if} \; \mathbf{a}[u] < 0 \\
    \end{cases}.
\end{equation}

\end{document}
