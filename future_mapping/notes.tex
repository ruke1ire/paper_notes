% pdflatex notes.tex 

\documentclass[twocolumn]{article}
\usepackage{amsfonts}
\usepackage{pifont}
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[most]{tcolorbox}
\usepackage{empheq}
\usepackage{geometry}
\geometry{a4paper, margin=1cm, includefoot}

\newtcbox{\mymath}[1][]{%
    nobeforeafter, math upper, tcbox raise base,
    enhanced, colframe=blue!30!black,
    colback=blue!15, boxrule=1pt,
    #1}

\begin{document}

\title{Notes on \href{https://arxiv.org/pdf/1910.14139.pdf}{FutureMapping 2: Gaussian Belief Propagation for Spatial AI}}
\author{Rom Parnichkun}

\maketitle

\section{Overview}

This note lays out the formulation of belief propagation, and subsequently Gaussian belief propagation (GBP). Both these algorithms take advantage of a graphical structure called \textbf{Factor Graphs}. Within it, there are \textbf{factor nodes} and \textbf{variable nodes}. Variable nodes contain variables that we would like to compute the marginal probability of, and factor nodes relates the various variable nodes together with a joint distribution.

\subsection{Example Problem}

Understanding this algorithm is difficult when only looking at the mathematical formulations and abstractions therefore this section provides concrete examples where the GBP algorithm may be used.

\begin{itemize}
    \item \textbf{Background:} There are two robots, robot A and robot B. Robot A measures the distance from itself to robot B with a sensor (which has Gaussian noise). Robot A also has a belief on its own position (characterized by a Gaussian distribution).
    \item \textbf{Problem:} Find the position of robot B.
    \item \textbf{Solution:} In order to find the position of robot B, we can simply add robot A's position with the relative distance between robot A and robot B from the sensor. However, there are distributions associated with both the position and the measurement, therefore we should view robot A and robot B's positions to be in a joint distribution (\textbf{factor node}), then marginalize robot B's position to get its distribution (belief).
\end{itemize}

The previous example illustrates a single message passing scheme, which does not fully take advantage of Gaussian belief propagation.

\begin{itemize}
    \item \textbf{Background:} Now instead of two robots imagine hundreds of robots. Each robot has the ability to measure another robot that is in its visibility range. They all also have a belief on its own position.
    \item \textbf{Problem:} Find the position of every robot so that they consistent with each other.
    \item \textbf{Solution:} Each measurement of another robot creates a new factor node. A factor node represents the flow of belief from one variable node to another. Therefore in this case, if there are multiple robots that have measurements of a robot, all the beliefs from the multiple robots are combined to update the belief of the robot being measured. This process iteratively updates the position of every robot in the graph and slowly converges to states that are consistent to one another in a distributed manner.
\end{itemize}

It is important to point out that GBP provides a way to create a globally consistent graph with distributed computation. Additionally, GBP supports ad-hoc message passing making it suitable for robotics applications.

\section{Tutorial on Belief Propagation}

Marginal distribution is found by taking the joint distribution, and summing over all the other variables:
\begin{equation}
    p(x) = \sum_{\textbf{x} \setminus x}{p(\textbf{x})}.
    \label{eq:marginal}
\end{equation}
The total joint probability is defined as the product of all factors in a graph: \footnote{Note that $p(\textbf{x})$ here may note be normalized.}
\begin{equation}
    p(\textbf{x}) = \prod_s{f_s(\textbf{x}_s)},
\end{equation}

Setting $F_s$ as the product of all factors associated with $f_s$ (a factor that is directly connnected to $x$) we get:
\begin{equation}
    p(\textbf{x}) = \prod_{s \in n(x)}{F_s(x, \textbf{X}_s)},
    \label{eq:joint}
\end{equation}
where $n(x)$ is the set of factor nodes that are neighbors of $x$ and $\textbf{X}_s$ is the vector of all variables in the subtree conntected to $x$ via $f_s$. Note that with equation \ref{eq:joint} we reduce the problem of finding the joint probability to a local one $s\in n(x)$.
 
Next, combining equations \ref{eq:marginal} and \ref{eq:joint} we get:
\begin{equation}
    p(x) = \sum_{\textbf{x} \setminus x}\Big[\prod_{s \in n(x)}{F_s(x, \textbf{X}_s)}\Big].
\end{equation}
We can reorder the sum and product to obtain:
\begin{equation}
    p(x) = \prod_{s \in n(x)}\Big[\sum_{\textbf{X}_s}{F_s(x, \textbf{X}_s)}\Big].
    \label{eq:reordered}
\end{equation}
Intuitively, this reordering is simply changing the marginalisation to be over a subtree, then multiplying over all the marginals to get the final marginal. It effectively decouples the marginalisation process to a subtree (from the summation over $\textbf{x} \setminus x$ to $\textbf{X}_s$) allowing us to define a ``message" from $f_s$ as shown below. \footnote{A message is like a local marginalisation.}
\begin{equation}
    \mu_{f_s \rightarrow x} = \sum_{\textbf{X}_s}{F_s(x,\textbf{X}_s)}.
    \label{eq:message_from_factor}
\end{equation}
Substituting Equation \ref{eq:message_from_factor} to \ref{eq:reordered} results in
\begin{empheq}[box=\mymath]{equation}
    p(x) = \prod_{s \in n(x)}\Big[\mu_{f_s \rightarrow x}\Big].
\end{empheq}
Next, we break down $F_s(x, \textbf{X}_s)$ as
\begin{multline}
F_s(x, \textbf{X}_s) = f_s(x,x_1,\dots,x_M)\\
\times \big[G_1(x_1,\textbf{X}_{S_1}),\dots, G_M(x_M,\textbf{X}_{S_M})  \big] \\
= f_s(x,x_1,\dots,x_M)\prod_{m\in n(f_s)} G_m(x_m, \textbf{X}_{S_m}),
\label{eq:f}
\end{multline}
in which the factor $f_s$ is a function of all nodes conntected to it, and $G_m(x_m, X_{S_m})$ is defined as follows.
\begin{equation}
    G_m(x_m, \textbf{X}_{S_m}) = \prod_{l \in n(x_m) \setminus f_s}{F_l(x_m, \textbf{X}_{m_l})}.
    \label{eq:g}
\end{equation}
Equation \ref{eq:g} shows the recursive nature of computing $F_s$ ($F \rightarrow G \rightarrow F \dots$), therefore we can reorganize and substitute equation \ref{eq:f} to equation \ref{eq:message_from_factor} to reflect this nature.
\begin{multline}
    \mu_{f_s \rightarrow x}(x) = \sum_{\textbf{X}_s}\Big[f_s(x,x_1,\dots,x_M) \prod_{m\in n(f_s)} G_m(x_m, \textbf{X}_{S_m})\Big]\\
    = \sum_{x_1,\dots,x_M}\sum_{\textbf{X}_{S_1},\dots,\textbf{X}_{S_M}}\Big[f_s(x,x_1,\dots,x_M) \prod_{m\in n(f_s)} G_m(x_m, \textbf{X}_{S_m})\Big]\\
    = \sum_{x_1,\dots,x_M}{f_s(x,x_1,\dots,x_M)}\prod_{m\in n(f_s)}\sum_{\textbf{X}_{S_1},\dots,\textbf{X}_{S_M}}G_m(x_m, \textbf{X}_{S_m})
    \label{eq:message_from_factor2}
\end{multline}
Similar to Equation \ref{eq:message_from_factor}, $G_m$ can be thought of as a message coming from the variable, hence we define the message $\mu_{x_m \rightarrow f_s}(x_m)$ as follows:
\begin{equation}
    \mu_{x_m \rightarrow f_s}(x_m) = \sum_{\textbf{X}_{S_m}}{G_m(x_m, \textbf{X}_{S_m})}
    \label{eq:message_from_variable}
\end{equation}
Substituting $\mu_{x_m \rightarrow f_s}$ to equation \ref{eq:message_from_factor2} we get
\begin{empheq}[box=\mymath]{multline}
    \mu_{f_s \rightarrow x}(x) = \sum_{x_1,\dots,x_M}{f_s(x,x_1,\dots,x_M)}\\
    \times \prod_{m\in n(f_s)}\mu_{x_m \rightarrow f_s}(x_m).
\end{empheq}
Next, we substitute Equation \ref{eq:g} to \ref{eq:message_from_variable} to get
\begin{equation}
    \mu_{x_m \rightarrow f_s}(x_m) = \sum_{\textbf{X}_{S_m}}\prod_{l\in n(x_m) \setminus f_s}{F_l(x_m, \textbf{X}_{m_l})},
\end{equation} 
and swap the order of the sum and product to obtain:
\begin{equation}
    \mu_{x_m \rightarrow f_s}(x_m) = \prod_{l\in n(x_m) \setminus f_s}\sum_{\textbf{X}_{m_l}}{F_l(x_m, \textbf{X}_{m_l})}.
\end{equation} 
Finally we derive the message from variables that completes the recursion 
\begin{empheq}[box=\mymath]{equation}
    \mu_{x_m \rightarrow f_s}(x_m) = \prod_{l\in n(x_m) \setminus f_s}\mu_{f_l \rightarrow x_m}{(x_m)}.
\end{empheq}

In order to find the marginal distribution for $x$, we start from all of the leaf nodes of the factor graph relative to $x$, and pass messages inwards torwards $x$. To initialise the leaf nodes, a variable leaf node sends a message $\mu_{x \rightarrow f}(x) = 1$ to its only connected factor and a leaf factor sends $\mu_{f \rightarrow x}(x) = f(x)$.

To find the marginal for \textit{all} variables, we send the messages outwards from the root back to the leaves.

\section{Gaussian Belief Propagation}

If the relationship between variables is linear, representing the probabilities of the variables with a multivariate Gaussian distribution enables the marginals to also be Gaussian distributions. 

\subsection{Factor Definition}

A Gaussian factor can be written as follows.
\begin{equation}
    f_s(\textbf{x}_s) = Ke^{-\frac{1}{2}[(\textbf{z}_s - \textbf{h}_s(\textbf{x}_s))^T\Lambda_s(\textbf{z}_s - \textbf{h}_s(\textbf{x}_s))]}.
\end{equation}
This expression represents the probability of obtaining measurement vector $\textbf{z}_s$ from the sensor based on the other variables $\textbf{x}_s$. $\textbf{h}_s$ is the function which describes the relationship between the variables on the expected measurement $\textbf{z}_s$. Matrix ${\bf\Lambda}_s$ is the precision of the measurement.

With the information form (canonical form), the Gaussian distrubution can be represented as follows.\footnote{For more information about the information form read \href{http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.140.3323&rep=rep1&type=pdf}{Exactly Sparse Delayed-State Filters}.}
\begin{equation}
    f_s(\textbf{x}_s) = Ke^{[-\frac{1}{2}\textbf{x}^T_s\boldsymbol{\Lambda}_s^{'}\textbf{x}_s + \boldsymbol{\eta}^T_s\textbf{x}_s]},
\end{equation}
where
\begin{equation}
    \boldsymbol{\eta}_s = \boldsymbol{\Lambda}_s \boldsymbol{\mu}_s.
\end{equation}

\subsection{State Representation}

The probability distribution over state variables also have a Gaussian form as follows.
\begin{equation}
    p_m(\textbf{x}_m) = Ke^{[-\frac{1}{2}\textbf{x}^T_m{\bf\Lambda}_m\textbf{x}_m + {\bf\eta}^T_m\textbf{x}_m]}.
\end{equation}

\subsection{Linearising Factors}

We can linearise any factor to the equation below
\begin{equation}
    f_s(\textbf{x}_s) = Ke^{[-\frac{1}{2}\textbf{x}^{T}_{s} \boldsymbol{\Lambda}^{'}_s\textbf{x}_s + \boldsymbol{\eta}_s^T\textbf{x}_s]}
\end{equation}
\begin{equation}
    \boldsymbol{\eta}_s = \textbf{J}^T_s\boldsymbol{\Lambda}_s[\textbf{J}_s\textbf{x}_0 + \textbf{z}_s - \textbf{h}_s(\textbf{x}_0)]
\end{equation}
\begin{equation}
    \boldsymbol{\Lambda}^{'}_s = \textbf{J}_s^T\boldsymbol{\Lambda}_s\textbf{J}_s.
\end{equation}
where $\textbf{J}_s$ is the Jacobian $\frac{\partial \textbf{h}_s}{\partial \textbf{x}_s} \vert_{\textbf{x}_s = \textbf{x}_0}$, $\textbf{x}_0$ is the point of linearisation.

\subsection{Message Passing at a Variable Node}

For GBP, each incoming message $\mu_{f_l \rightarrow \textbf{x}_m}(\textbf{x}_m)$ is represented by an information vector $\boldsymbol{\eta}_{ml}$ and a precision matrix $\boldsymbol{\Lambda}_{ml}$.
The outgoing message $\mu_{\textbf{x}_m \rightarrow f_s }(\textbf{x}_m)$ is computed by simply adding 
\begin{equation}
    \boldsymbol{\eta}_{ms} = \sum_{l \in n(\textbf{x}_m) \setminus f_s}{\boldsymbol{\eta}_{ml}}
\end{equation}
\begin{equation}
    \boldsymbol{\Lambda}_{ms} = \sum_{l \in n(\textbf{x}_m) \setminus f_s}{\boldsymbol{\Lambda}_{ml}}.
\end{equation}
This is equivalent to multiplying the distributions.



\end{document}
